from datetime import date, timedelta
from pyspark.sql import SparkSession
from pyspark import RDD
import pytest

spark = SparkSession.builder.getOrCreate()

def reduce_per_user(rdd: RDD, ticker: int) -> RDD:
    """Converts raw rows into one row per user.

    Each user row is in the form of (user_id, list[(data,)])
    """
    # filter by the ticker and then drop it
    rdd = rdd.filter(lambda x: x[2] == ticker)
    rdd = rdd.map(lambda x: (x[0], [(x[1], x[3])]))

    def merge(list1: list[tuple], list2: list[tuple]) -> list[tuple]:
        return list1 + list2

    return rdd.reduceByKey(merge)


def run_report(rdd: RDD, ticker: int) -> RDD:
    rdd = reduce_per_user(rdd, ticker)

    return rdd

if __name__ == "__main__":
    # data for 3 users
    data = [
        (1, date(2021, 1, 1), 7203, 10),
        (1, date(2021, 1, 1), 3382, 30),
        (2, date(2021, 1, 3), 3382, 65),
        (1, date(2021, 1, 4), 7203, 20),
        (2, date(2021, 1, 9), 3382, 75),
        (1, date(2021, 1, 10), 7203, 30),
        (1, date(2021, 1, 10), 3382, 40),
        (3, date(2021, 1, 15), 3382, 7),
        (2, date(2021, 1, 19), 3382, 2),
    ]
    rdd = spark.sparkContext.parallelize(data)
    ticker = 3382
    report = run_report(rdd, ticker)

    print(list(report.collect()))
